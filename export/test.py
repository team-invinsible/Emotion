#!/usr/bin/env python
# coding: utf-8

import cv2
import torch
from torchvision import transforms
from PIL import Image, ImageFont, ImageDraw
from models import getModel  
import os
import time  # FPS ì¸¡ì •ì„ ìœ„í•´ ì¶”ê°€
import glob
from collections import defaultdict
import multiprocessing as mp
from concurrent.futures import ProcessPoolExecutor
import threading
import numpy as np

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€” ì‚¬ìš©ì ì„¤ì • â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
# ë‹¤ì¤‘ ë™ì˜ìƒ ì²˜ë¦¬ ì„¤ì •
# VIDEO_FOLDER = '/Users/ijaein/Desktop/Emotion/export/video/'  # ë¹„ë””ì˜¤ í´ë” ê²½ë¡œ
VIDEO_PATH   = '/Users/ijaein/Desktop/Emotion/export/video/ì´ìŠ¹ë¬´ì›.mp4' # ë‹¨ì¼ ë¹„ë””ì˜¤ ê²½ë¡œ (ì˜ˆì‹œ)
MODEL_PATH   = '/Users/ijaein/Desktop/Emotion/model_eff.pth'  # EfficientNet ëª¨ë¸ ê°€ì¤‘ì¹˜ íŒŒì¼
CASCADE_PATH = '/Users/ijaein/Desktop/Emotion/export/face_classifier.xml'
MODEL_NAME   = 'efficientnet-b5'  # EfficientNet-b5 ì‚¬ìš©
IMAGE_SIZE   = 224  

# ì²˜ë¦¬ ë°©ì‹ ì„¤ì •
PARALLEL_PROCESSING = False  # ë‹¨ì¼ ë¹„ë””ì˜¤ëŠ” ìˆœì°¨ ì²˜ë¦¬
MAX_WORKERS = 4              # ë³‘ë ¬ ì²˜ë¦¬ì‹œ ìµœëŒ€ ì›Œì»¤ ìˆ˜ (CPU ì½”ì–´ìˆ˜ì— ë§ê²Œ ì¡°ì •)
SHOW_VIDEO = True            # ë™ì˜ìƒ í™”ë©´ í‘œì‹œ ì—¬ë¶€ (ë‹¨ì¼ ë¹„ë””ì˜¤ëŠ” True ê¶Œì¥)

# ì†ë„ ìµœì í™” ì„¤ì • (ì‹œê°„ ê¸°ë°˜ ë²„ì „)
ANALYSIS_INTERVAL = 1.0  # 1ì´ˆë§ˆë‹¤ 1ë²ˆ ë¶„ì„
PLAYBACK_SPEED = 5     # ë¹„ë””ì˜¤ ì¬ìƒ ì†ë„ (2.5ë°°ì†)

# ì¶”ê°€ ìµœì í™” ì˜µì…˜ë“¤
FAST_FACE_DETECTION = True  # ë¹ ë¥¸ ì–¼êµ´ ê²€ì¶œ ëª¨ë“œ
USE_LIGHTER_MODEL = False   # ë” ê°€ë²¼ìš´ CNN ëª¨ë¸ ì‚¬ìš© (Trueë¡œ ì„¤ì •í•˜ë©´ CNN ì‚¬ìš©)

# í•œê¸€ ë¼ë²¨ 
class_labels = ['ê¸°ì¨', 'ë‹¹í™©', 'ë¶„ë…¸', 'ë¶ˆì•ˆ', 'ìƒì²˜', 'ìŠ¬í””', 'ì¤‘ë¦½']

# ê°ì • ë§¤í•‘ (í‰ê°€ìš©)
EMOTION_MAPPING = {
    'ê¸°ì¨': 'happy',
    'ë‹¹í™©': 'surprise', 
    'ë¶„ë…¸': 'angry',
    'ë¶ˆì•ˆ': 'fear',
    'ìƒì²˜': 'disgust',
    'ìŠ¬í””': 'sad',
    'ì¤‘ë¦½': 'neutral'
}

# ë©´ì ‘ í‰ê°€ ê¸°ì¤€
POSITIVE_EMOTIONS = ['happy', 'neutral']
NEGATIVE_EMOTIONS = ['sad', 'angry', 'fear', 'surprise', 'disgust']

# í°íŠ¸ ì„¤ì • (AppleGothic.ttf ê²½ë¡œ ë° í¬ê¸°)
FONT_PATH = '/System/Library/Fonts/AppleGothic.ttf' # AppleGothic í°íŠ¸ ê²½ë¡œ (macOS ê¸°ë³¸ ê²½ë¡œ)
FONT_SIZE = 30 # í°íŠ¸ í¬ê¸°

# â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”


def get_video_files(folder_path):
    """ë¹„ë””ì˜¤ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜"""
    video_extensions = ['*.mp4', '*.avi', '*.mov', '*.mkv', '*.flv', '*.wmv', '*.m4v', '*.webm']
    
    if not os.path.isdir(folder_path):
        print(f"âŒ í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {folder_path}")
        return []
    
    video_files = set()  # ì¤‘ë³µ ì œê±°ë¥¼ ìœ„í•´ set ì‚¬ìš©
    for ext in video_extensions:
        # í˜„ì¬ í´ë”ë§Œ ê²€ìƒ‰ (í•˜ìœ„ í´ë” ì œì™¸ë¡œ ì¤‘ë³µ ë°©ì§€)
        video_files.update(glob.glob(os.path.join(folder_path, ext)))
    
    return sorted(list(video_files))


def calculate_interview_score(emotion_data):
    """ë©´ì ‘ í‰ê°€ ì ìˆ˜ ê³„ì‚° í•¨ìˆ˜"""
    if not emotion_data:
        return 0, {}
    
    # ê°ì •ë³„ í†µê³„ ê³„ì‚°
    total_frames = len(emotion_data)
    emotion_counts = defaultdict(int)
    emotion_confidences = defaultdict(list)
    emotion_transitions = 0
    prev_emotion = None
    
    for frame_data in emotion_data:
        emotion = frame_data['emotion']
        confidence = frame_data['confidence']
        
        emotion_counts[emotion] += 1
        emotion_confidences[emotion].append(confidence)
        
        # ê°ì • ì „ì´ ê³„ì‚°
        if prev_emotion is not None and prev_emotion != emotion:
            emotion_transitions += 1
        prev_emotion = emotion
    
    # 1. ê¸ì • ê°ì • ë¹„ìœ¨ (ìµœëŒ€ 25ì )
    positive_count = sum(emotion_counts[emotion] for emotion in POSITIVE_EMOTIONS)
    positive_ratio = positive_count / total_frames
    positive_score = positive_ratio * 100 * 0.25
    
    # 2. ë¶€ì • ê°ì • ë¹„ìœ¨ (ìµœëŒ€ 15ì )
    negative_count = sum(emotion_counts[emotion] for emotion in NEGATIVE_EMOTIONS)
    negative_ratio = negative_count / total_frames
    negative_score = (1 - negative_ratio) * 100 * 0.15
    
    # 3. happy í‰ê·  confidence (ìµœëŒ€ 20ì )
    if 'happy' in emotion_confidences and emotion_confidences['happy']:
        happy_confidence = sum(emotion_confidences['happy']) / len(emotion_confidences['happy'])
        happy_score = happy_confidence * 100 * 0.2
    else:
        happy_confidence = 0
        happy_score = 0
    
    # ì´ì  ê³„ì‚° (60ì  ë§Œì )
    total_score = min(60, positive_score + negative_score + happy_score)
    
    # ìƒì„¸ ë¶„ì„ ê²°ê³¼
    analysis = {
        'total_frames': total_frames,
        'emotion_counts': dict(emotion_counts),
        'emotion_ratios': {emotion: count/total_frames for emotion, count in emotion_counts.items()},
        'positive_ratio': positive_ratio,
        'negative_ratio': negative_ratio,
        'happy_confidence': happy_confidence,
        'emotion_transitions': emotion_transitions,
        'scores': {
            'positive_score': positive_score,
            'negative_score': negative_score, 
            'happy_score': happy_score,
            'total_score': total_score
        }
    }
    
    return total_score, analysis


def print_interview_report(video_name, analysis):
    """ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ì¶œë ¥"""
    print(f"\nğŸ“Š === {video_name} ë©´ì ‘ í‰ê°€ ë¦¬í¬íŠ¸ ===")
    print("=" * 60)
    
    scores = analysis['scores']
    print(f"ğŸ¯ **ìµœì¢… ì ìˆ˜: {scores['total_score']:.1f}/60ì **")
    print(f"ğŸ“ˆ **í‰ê°€ ë“±ê¸‰: {get_grade(scores['total_score'])}**")
    
    print(f"\nğŸ“‹ **ì„¸ë¶€ í‰ê°€:**")
    print(f"  1. ê¸ì • ê°ì • ë¹„ìœ¨: {scores['positive_score']:.1f}/25ì  ({analysis['positive_ratio']*100:.1f}%)")
    print(f"  2. ë¶€ì • ê°ì • ì œì–´: {scores['negative_score']:.1f}/15ì  ({analysis['negative_ratio']*100:.1f}%)")
    print(f"  3. ë¯¸ì†Œ ì‹ ë¢°ë„: {scores['happy_score']:.1f}/20ì  ({analysis['happy_confidence']*100:.1f}%)")
    
    print(f"\nğŸ“ˆ **ê°ì • ë¶„í¬:**")
    for emotion, count in analysis['emotion_counts'].items():
        ratio = analysis['emotion_ratios'][emotion]
        print(f"  - {emotion}: {count}íšŒ ({ratio*100:.1f}%)")
    
    print(f"\nğŸ’¡ **ê°œì„  ì œì•ˆ:**")
    suggestions = get_improvement_suggestions(analysis)
    for suggestion in suggestions:
        print(f"  â€¢ {suggestion}")
    
    print("=" * 60)


def get_grade(score):
    """ì ìˆ˜ë¥¼ ë“±ê¸‰ìœ¼ë¡œ ë³€í™˜ (60ì  ë§Œì  ê¸°ì¤€)"""
    if score >= 54:  # 90% ì´ìƒ
        return "A+ (ìš°ìˆ˜)"
    elif score >= 48:  # 80% ì´ìƒ
        return "A (ì–‘í˜¸)"
    elif score >= 42:  # 70% ì´ìƒ
        return "B+ (ë³´í†µ)"
    elif score >= 36:  # 60% ì´ìƒ
        return "B (ë¯¸í¡)"
    else:
        return "C (ê°œì„  í•„ìš”)"


def get_improvement_suggestions(analysis):
    """ê°œì„  ì œì•ˆ ìƒì„±"""
    suggestions = []
    scores = analysis['scores']
    
    if scores['positive_score'] < 15:
        suggestions.append("ë” ìì£¼ ë¯¸ì†Œë¥¼ ì§“ê³  ê¸ì •ì ì¸ í‘œì •ì„ ìœ ì§€í•˜ì„¸ìš”")
    
    if scores['negative_score'] < 12:
        suggestions.append("ë¶€ì •ì ì¸ ê°ì • í‘œí˜„ì„ ì¤„ì´ê³  ì¤‘ë¦½ì ì¸ í‘œì •ì„ ìœ ì§€í•˜ì„¸ìš”")
    
    if scores['happy_score'] < 15:
        suggestions.append("ë¯¸ì†Œì˜ ì§„ì •ì„±ì„ ë†’ì´ê³  ìì—°ìŠ¤ëŸ¬ìš´ í‘œì •ì„ ì—°ìŠµí•˜ì„¸ìš”")
    
    if not suggestions:
        suggestions.append("ì „ë°˜ì ìœ¼ë¡œ ìš°ìˆ˜í•œ í‘œì • ê´€ë¦¬ë¥¼ ë³´ì—¬ì£¼ì—ˆìŠµë‹ˆë‹¤!")
    
    return suggestions


def build_model(model_name, ckpt_path):
    # ëª¨ë¸ ìƒì„±
    model = getModel(model_name)
    
    # ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ (ê°€ì¤‘ì¹˜ íŒŒì¼ì´ ìˆëŠ” ê²½ìš°ë§Œ)
    try:
        if ckpt_path and os.path.exists(ckpt_path):
            ckpt = torch.load(ckpt_path, map_location='cpu')
            state = ckpt.get('model', ckpt)
            model.load_state_dict(state)
            print(f"âœ“ ê°€ì¤‘ì¹˜ë¥¼ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {ckpt_path}")
        else:
            print(f"âš ï¸ ê°€ì¤‘ì¹˜ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    except Exception as e:
        print(f"âš ï¸ ê°€ì¤‘ì¹˜ ë¡œë”© ì‹¤íŒ¨: {str(e)}")
        print("ëœë¤ ì´ˆê¸°í™”ëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.")
    
    model.eval()
    return model


def process_single_video(video_path):
    """ë‹¨ì¼ ë¹„ë””ì˜¤ ì²˜ë¦¬ í•¨ìˆ˜ (ë³‘ë ¬ ì²˜ë¦¬ìš©)"""
    try:
        print(f"ğŸ¬ ì²˜ë¦¬ ì‹œì‘: {os.path.basename(video_path)}")
        
        # ëª¨ë¸ ë¡œë“œ (ê° í”„ë¡œì„¸ìŠ¤ë§ˆë‹¤)
        if USE_LIGHTER_MODEL:
            model_name = 'cnn'
            model_path = '/Users/ijaein/Desktop/Emotion/export/model.pth'
            image_size = 48
        else:
            model_name = MODEL_NAME
            model_path = MODEL_PATH
            image_size = IMAGE_SIZE
        
        model = build_model(model_name, model_path)
        
        # ì „ì²˜ë¦¬
        if USE_LIGHTER_MODEL:
            transform = transforms.Compose([
                transforms.Grayscale(num_output_channels=1),
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize([0.485], [0.229])
            ])
        else:
            transform = transforms.Compose([
                transforms.Resize((image_size, image_size)),
                transforms.ToTensor(),
                transforms.Normalize([0.485, 0.456, 0.406],
                                   [0.229, 0.224, 0.225])
            ])
        
        # ì–¼êµ´ ê²€ì¶œê¸°
        face_cascade = cv2.CascadeClassifier(CASCADE_PATH)
        if FAST_FACE_DETECTION:
            scale_factor = 1.2
            min_neighbors = 4
        else:
            scale_factor = 1.1
            min_neighbors = 5
        
        # ë¹„ë””ì˜¤ ì²˜ë¦¬
        return process_video_core(video_path, model, transform, face_cascade, 
                                scale_factor, min_neighbors, image_size)
    
    except Exception as e:
        print(f"âŒ {os.path.basename(video_path)} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {str(e)}")
        return None


def process_video_core(video_path, model, transform, face_cascade, scale_factor, min_neighbors, image_size):
    """ë¹„ë””ì˜¤ ì²˜ë¦¬ í•µì‹¬ ë¡œì§"""
    cap = cv2.VideoCapture(video_path)
    if not cap.isOpened():
        print(f"âŒ Cannot open video: {video_path}")
        return None

    # ì†ë„ ìµœì í™” ë³€ìˆ˜ë“¤
    frame_count = 0
    processed_frames = 0  # ì‹¤ì œ ì²˜ë¦¬ëœ í”„ë ˆì„ ìˆ˜
    
    # ê°ì • ë°ì´í„° ìˆ˜ì§‘
    emotion_data = []
    
    # FPS ì¸¡ì •ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
    start_time = time.time()
    fps_start_time = time.time()
    fps_frame_count = 0
    current_fps = 0
    
    # ì¬ìƒ ì†ë„ ì¡°ì ˆì„ ìœ„í•œ ì§€ì—° ì‹œê°„ ê³„ì‚°
    fps = cap.get(cv2.CAP_PROP_FPS) or 30
    delay = max(1, int(1000 / (fps * PLAYBACK_SPEED)))
    
    # 1ì´ˆì— í•´ë‹¹í•˜ëŠ” í”„ë ˆì„ ìˆ˜ ê³„ì‚° (ì§„ì§œ ì‹œê°„ ê¸°ë°˜)
    frames_per_interval = int(fps * ANALYSIS_INTERVAL)
    
    print(f"ì›ë³¸ ë¹„ë””ì˜¤ FPS: {fps}")
    print(f"ë¶„ì„ ê°„ê²©: {ANALYSIS_INTERVAL}ì´ˆ = {frames_per_interval} í”„ë ˆì„ë§ˆë‹¤")
    print(f"ì´ë¡ ì  ì²˜ë¦¬ FPS: {fps / frames_per_interval:.1f}")
    print(f"ì¬ìƒ ì†ë„ ì ìš©: {fps / frames_per_interval * PLAYBACK_SPEED:.1f} (ì²´ê° FPS)")
    print("-" * 50)

    # í°íŠ¸ ë¡œë“œ
    try:
        font = ImageFont.truetype(FONT_PATH, FONT_SIZE)
        # ì‘ì€ í°íŠ¸ (ì„±ëŠ¥ ì •ë³´ìš©)
        font_small = ImageFont.truetype(FONT_PATH, FONT_SIZE - 10) 
    except IOError:
        print(f"âš ï¸ í°íŠ¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ê±°ë‚˜ ë¡œë“œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {FONT_PATH}. ê¸°ë³¸ í°íŠ¸ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.")
        font = ImageFont.load_default()
        font_small = ImageFont.load_default()

    with torch.no_grad():
        while True:
            ret, frame = cap.read()
            if not ret:
                break

            frame_count += 1
            
            # 1ì´ˆ ê°„ê²© í”„ë ˆì„ ìŠ¤í‚µ ì ìš© (ì§„ì§œ ì‹œê°„ ê¸°ë°˜)
            if frame_count % frames_per_interval != 0:
                continue

            processed_frames += 1
            fps_frame_count += 1
            
            # FPS ê³„ì‚° (1ì´ˆë§ˆë‹¤ ì—…ë°ì´íŠ¸)
            current_time = time.time()
            if current_time - fps_start_time >= 1.0:
                current_fps = fps_frame_count / (current_time - fps_start_time)
                fps_start_time = current_time
                fps_frame_count = 0

            # (ì›¹ìº ìš©ì´ë¼ë©´ ì¢Œìš° ë°˜ì „, ë™ì˜ìƒì´ë¼ë©´ ì£¼ì„ ì²˜ë¦¬)
            # frame = cv2.flip(frame, 1) # ì´ì „ì— ì£¼ì„ ì²˜ë¦¬ë˜ì–´ ìˆì§€ ì•Šì•˜ë‹¤ë©´ ì´ ì¤„ë„ ì£¼ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.

            # Convert frame to PIL Image to draw text
            img_pil = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
            draw = ImageDraw.Draw(img_pil)

            # ì–¼êµ´ ê²€ì¶œ (ë§¤ë²ˆ ìˆ˜í–‰ - ë‹¨ìˆœí™”)
            gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
            detected_faces = face_cascade.detectMultiScale(gray, scale_factor, min_neighbors)
            
            # ì¤‘ë³µ ì–¼êµ´ ì œê±° ë° ê°€ì¥ í° ì–¼êµ´ë§Œ ì„ íƒ
            if len(detected_faces) > 0:
                # ì–¼êµ´ í¬ê¸°(ë©´ì ) ê¸°ì¤€ìœ¼ë¡œ ì •ë ¬í•˜ì—¬ ê°€ì¥ í° ì–¼êµ´ë§Œ ì„ íƒ
                faces_with_area = [(x, y, w, h, w*h) for x, y, w, h in detected_faces]
                faces_with_area.sort(key=lambda x: x[4], reverse=True)  # ë©´ì  ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ
                
                # ê°€ì¥ í° ì–¼êµ´ë§Œ ì„ íƒ (ì¤‘ë³µ ì œê±°)
                largest_face = faces_with_area[0]
                faces = [(largest_face[0], largest_face[1], largest_face[2], largest_face[3])]
            else:
                faces = []

            # ì–¼êµ´ í•˜ë‚˜ì”© ì²˜ë¦¬
            for i, (x, y, w, h) in enumerate(faces):
                # ROI ìë¥´ê³  PILâ†’Tensor
                face = frame[y:y+h, x:x+w]
                face = cv2.cvtColor(face, cv2.COLOR_BGR2RGB)
                pil = Image.fromarray(cv2.resize(face, (image_size, image_size)))
                inp = transform(pil).unsqueeze(0)

                # ì˜ˆì¸¡
                logits = model(inp)
                probs  = torch.softmax(logits, dim=1)[0]
                p, idx = probs.max(0)
                emotion_korean = class_labels[idx]
                emotion_english = EMOTION_MAPPING[emotion_korean]
                label = f"{emotion_korean} ({p*100:.1f}%)"

                # ê°ì • ë°ì´í„° ì €ì¥
                emotion_data.append({
                    'frame': frame_count,
                    'emotion': emotion_english,
                    'emotion_korean': emotion_korean,
                    'confidence': p.item()
                })

                # ì½˜ì†”ì—ë„ ì¶œë ¥
                print(f"[Frame {frame_count}] face#{i}: {label}")

                # í™”ë©´ì— ì¶œë ¥ (PIL ì‚¬ìš©)
                # ì–¼êµ´ ë°”ìš´ë”© ë°•ìŠ¤ ê·¸ë¦¬ê¸° (PIL ì‚¬ìš©)
                draw.rectangle([(x, y), (x+w, y+h)], outline=(0, 255, 0), width=2) # ì´ˆë¡ìƒ‰ í…Œë‘ë¦¬
                
                # í…ìŠ¤íŠ¸ ìœ„ì¹˜ ê³„ì‚° (ì–¼êµ´ ìœ„) - textbbox ì‚¬ìš©
                bbox = draw.textbbox((0, 0), label, font=font)
                text_width = bbox[2] - bbox[0]
                text_height = bbox[3] - bbox[1]
                text_x = x
                text_y = y - text_height - 5 # ì–¼êµ´ ìœ„ 5í”½ì…€
                if text_y < 0: # í™”ë©´ ìƒë‹¨ ë²—ì–´ë‚˜ë©´ ì–¼êµ´ ì•„ë˜ë¡œ
                    text_y = y + h + 5

                draw.text((text_x, text_y), label, font=font, fill=(0, 255, 0, 255)) # ì´ˆë¡ìƒ‰ í…ìŠ¤íŠ¸ (RGBA)

            if len(faces) == 0:
                draw.text((20, 60), 'No Face Found', font=font, fill=(255, 0, 0, 255)) # ë¹¨ê°„ìƒ‰ í…ìŠ¤íŠ¸

            # ì„±ëŠ¥ ì •ë³´ í‘œì‹œ (PIL ì‚¬ìš©)
            draw.text((20, 30), f'Analysis Interval: {ANALYSIS_INTERVAL}s', font=font_small, fill=(0, 255, 255, 255)) # ì²­ë¡ìƒ‰ í…ìŠ¤íŠ¸
            draw.text((20, 60), f'Processing FPS: {current_fps:.1f}', font=font_small, fill=(0, 255, 255, 255))
            draw.text((20, 90), f'Processed: {processed_frames}/{frame_count}', font=font_small, fill=(0, 255, 255, 255))

            # PIL Imageë¥¼ ë‹¤ì‹œ OpenCV (NumPy ë°°ì—´)ë¡œ ë³€í™˜
            frame = cv2.cvtColor(np.array(img_pil), cv2.COLOR_RGB2BGR)

            # í™”ë©´ í‘œì‹œ (ì„ íƒì )
            if SHOW_VIDEO:
                cv2.imshow(f'Emotion Analysis - {os.path.basename(video_path)}', frame)
                # ì¬ìƒ ì†ë„ ì¡°ì ˆëœ ëŒ€ê¸° ì‹œê°„
                if cv2.waitKey(delay) & 0xFF == ord('q'):
                    break

    cap.release()
    if SHOW_VIDEO:
        cv2.destroyAllWindows()
    
    # ìµœì¢… í†µê³„
    total_time = time.time() - start_time
    average_fps = processed_frames / total_time if total_time > 0 else 0
    
    print(f"âœ… {os.path.basename(video_path)} ì™„ë£Œ!")
    print(f"   ì²˜ë¦¬ì‹œê°„: {total_time:.1f}ì´ˆ, í”„ë ˆì„: {processed_frames}/{frame_count}, FPS: {average_fps:.1f}")
    
    # ë©´ì ‘ í‰ê°€ ì ìˆ˜ ê³„ì‚°
    interview_score = 0
    interview_analysis = {}
    if emotion_data:
        interview_score, interview_analysis = calculate_interview_score(emotion_data)
        print_interview_report(os.path.basename(video_path), interview_analysis)
    
    # ê²°ê³¼ ë°˜í™˜
    return {
        'video_path': video_path,
        'total_time': total_time,
        'total_frames': frame_count,
        'processed_frames': processed_frames,
        'average_fps': average_fps,
        'emotion_data': emotion_data,
        'interview_score': interview_score,
        'interview_analysis': interview_analysis,
        'success': True
    }


def main(video_path=None):
    """ë©”ì¸ í•¨ìˆ˜ - ë‹¨ì¼ ë™ì˜ìƒ ì²˜ë¦¬"""
    print("ğŸ¬ ë‹¨ì¼ ë™ì˜ìƒ ê°ì • ë¶„ì„ ì‹œì‘")
    print("=" * 70)
    
    # ì„¤ì • ì¶œë ¥
    device = 'cpu'
    print(f"Using device: {device}")
    print(f"ìµœì í™” ì„¤ì •:")
    print(f"  - ë¶„ì„ ê°„ê²©: {ANALYSIS_INTERVAL}ì´ˆ")
    print(f"  - ì¬ìƒì†ë„: {PLAYBACK_SPEED}x")
    print(f"  - í™”ë©´ í‘œì‹œ: {SHOW_VIDEO}")
    print(f"  - ë¹ ë¥¸ ì–¼êµ´ê²€ì¶œ: {FAST_FACE_DETECTION}")
    print(f"  - ê°€ë²¼ìš´ ëª¨ë¸: {USE_LIGHTER_MODEL}")

    # ë¹„ë””ì˜¤ ê²½ë¡œ ì„¤ì •
    if video_path is None:
        video_path = VIDEO_PATH # ê¸°ë³¸ ê²½ë¡œ ì‚¬ìš©

    if not os.path.exists(video_path):
        print(f"âŒ ë™ì˜ìƒ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {video_path}")
        return
    
    print(f"\nğŸ“ ì²˜ë¦¬í•  ë™ì˜ìƒ íŒŒì¼: {os.path.basename(video_path)}")
    file_size = os.path.getsize(video_path) / (1024*1024)  # MB
    print(f"  í¬ê¸°: {file_size:.1f}MB")

    # ì²˜ë¦¬ ì‹œì‘
    total_start_time = time.time()
    result = process_single_video(video_path)
    
    # ì „ì²´ ê²°ê³¼ ìš”ì•½
    total_end_time = time.time()
    total_processing_time = total_end_time - total_start_time
    
    print("\n" + "=" * 70)
    print("ğŸ¯ ì²˜ë¦¬ ê²°ê³¼ ìš”ì•½")
    print("=" * 70)
    print(f"ì´ ì²˜ë¦¬ ì‹œê°„: {total_processing_time:.1f}ì´ˆ")
    
    if result and result['success']:
        video_name = os.path.basename(result['video_path'])
        print(f"  {video_name[:40]+'...' if len(video_name) > 40 else video_name}")
        print(f"     ì²˜ë¦¬ì‹œê°„: {result['total_time']:.1f}ì´ˆ, "
              f"í”„ë ˆì„: {result['processed_frames']}/{result['total_frames']}, "
              f"FPS: {result['average_fps']:.1f}")
        
        # ë©´ì ‘ ì ìˆ˜ í‘œì‹œ
        if 'interview_score' in result and result['interview_score'] > 0:
            score = result['interview_score']
            grade = get_grade(score)
            print(f"     ğŸ¯ ë©´ì ‘ì ìˆ˜: {score:.1f}/60ì  ({grade})")
    else:
        print("âŒ ë™ì˜ìƒ ì²˜ë¦¬ ì‹¤íŒ¨")
    
    print("ğŸ ì²˜ë¦¬ ì™„ë£Œ!")


if __name__ == "__main__":
    # ì—¬ê¸°ì—ì„œ ì²˜ë¦¬í•  ë‹¨ì¼ ë™ì˜ìƒ íŒŒì¼ ê²½ë¡œë¥¼ ì§€ì •í•˜ê±°ë‚˜, ê¸°ë³¸ê°’(VIDEO_PATH)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
    # ì˜ˆì‹œ: main('/Users/ijaein/Desktop/Emotion/export/video/my_interview.mp4')
    main()